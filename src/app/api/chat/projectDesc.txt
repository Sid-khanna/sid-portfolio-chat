
### **ParSight (capstone)**

As part of our final-year engineering capstone, we developed **ParSight**, an autonomous drone system designed to assist senior golfers by tracking golf balls in real time and hovering over their final position to provide a clear visual reference. The project addressed a growing accessibility challenge in recreational sports—namely, age-related vision decline—by offering a real-time robotic solution that enhances visibility without interfering with gameplay.

ParSight follows a **Sense–Plan–Act** architecture. Its sensing module features a downward-facing RGB camera mounted on a quadrotor drone, capturing video at 30 FPS. Frames are processed onboard via an NVIDIA Jetson Nano running ROS2. Real-time detection is achieved using HSV color filtering and OpenCV-based contour detection. The system scores contours based on size and circularity to isolate the red golf ball even in noisy or cluttered environments.

Tracking is managed using **Image-Based Visual Servoing (IBVS)**, where the pixel offset between the ball and the image center is converted into a physical setpoint via a Proportional-Derivative (PD) controller. These setpoints are published to the drone’s flight controller, which adjusts motor output to maintain visual lock and ultimately hover over the ball’s final location.

The system was deployed on a **Minion Mini H-Quad drone** equipped with a Jetson Nano and Orange Cube+ flight controller. It achieved **95% detection accuracy** with a false-positive rate under 3%, real-time inference at 30 Hz, and total system latency of approximately 350 ms. During MVP testing, the drone successfully hovered within **6 cm** of the ball’s final resting position—even under dynamic launch conditions using an RC car and a catapult.

ParSight met 8 of 10 key performance benchmarks, validating the feasibility of lightweight, vision-based autonomy for assistive applications. Planned upgrades include higher-frame-rate cameras, upgraded compute platforms (e.g., Jetson Orin), and YOLOv11-based deep learning models deployed with TensorRT for more robust outdoor tracking.

Great — here’s the revised version of the second project: **AI Story Generator**.


### **AI Story Generator**

As someone who loves writing—and is currently in the process of self-publishing a novel on Amazon—storytelling has always been one of my most meaningful creative outlets. At the same time, as an engineering student, I’ve been immersed in designing structured, logic-driven systems. This project became the perfect intersection of those two worlds.

The **AI Story Generator** is a web-based tool that guides users through a multi-step form to define a character’s voice, traits, tone, and genre. It then uses OpenRouter’s LLM API (such as DeepSeek or Mistral) to generate a rich, tailored character profile alongside a unique story blurb. Whether the genre is fantasy, sci-fi, or mystery, the tool adapts to user input to create engaging, genre-appropriate results.

Built with **Flask** and styled using **Tailwind CSS**, the app features a responsive UI, structured prompt logic, and clean, dialogue-styled prose output for immersive storytelling. It’s designed to feel like a co-creator—someone sitting beside you, helping brainstorm, write, and imagine.

This project highlights both my technical skills in web development and API integration, and my creative instincts as a writer. It reflects my belief that engineering and creativity are not opposing forces, but complementary strengths that can elevate one another. As I continue to explore AI-driven creativity, I plan to expand the platform with visual generation tools and longer-form story modes.


### **ViT + LLM Recipe Generator**

**ingrAIdients** is a completed deep learning project that reimagines how we interact with food using AI. The system allows users to upload a photo of a prepared dish and receive a breakdown of its ingredients, along with a fully generated recipe. Designed to support dietary tracking, allergen awareness, and creative meal recreation, it combines visual understanding with natural language generation to deliver a seamless user experience.

To train the model, we used large-scale datasets such as **Recipe1M+** and the **Food Ingredients and Recipe Dataset**, which provide millions of food images paired with detailed recipes. To improve generalization across a wider variety of cuisines and presentation styles, we also built a custom dataset via web scraping—collecting diverse images and their corresponding ingredient lists to ensure performance across underrepresented and culturally distinct meals.

**Deep Learning Architecture**

We experimented with multiple architectures to determine the most effective setup for ingredient recognition and recipe generation:

* **Convolutional Neural Networks (CNNs)**:
  Initial trials with **ResNet-50** and **VGG-16** established baseline visual features. While effective at extracting core image embeddings, they struggled to generalize across visually complex dishes.

* **Recurrent Neural Networks (RNNs)**:
  For ingredient list modeling, we used **GRUs** and **bi-directional LSTMs**, enabling multi-label predictions from CNN-generated feature vectors and supporting early ingredient detection.

* **Vision Transformers (ViT)**:
  We adopted **ViT-B/16** as the final visual backbone. It significantly outperformed previous methods by capturing both spatial and contextual details in food images, yielding better ingredient predictions on diverse input types.

* **Large Language Model (LLM)**:
  After identifying ingredients, we integrated an LLM to generate complete recipes—including measurements, cooking steps, and variations—based solely on the predicted ingredient list. This turned ingrAIdients into a true end-to-end food intelligence pipeline.

* **Joint Embedding Space (Planned)**:
  We also began exploring a joint embedding space between image features and ingredient text for future alignment improvements between visual and language models, though this has not been fully implemented.

**ingrAIdients** demonstrates the power of combining cutting-edge visual and language models to create intelligent, intuitive food applications. By blending curated and custom datasets with deep learning innovation, the project highlights how AI can make real-world impact in areas like health, nutrition, and culinary creativity.



### **Formula 1 Predictor**

As a lifelong Formula 1 fan—having followed the sport since I was six—I set out to build a personal machine learning project that predicts the outcomes of upcoming races. This project merges my passion for F1 with my interest in data analysis and model optimization, allowing me to explore how statistical trends can translate into competitive insights.

The model is trained on historical data including **driver performance**, **team statistics**, and **track history**. I used **Python**, **pandas**, and **TensorFlow** to preprocess and analyze the data, extracting key features such as average qualifying position, constructor momentum, and track-specific strengths.

To fine-tune model performance, I incorporated **Keras Tuner** for hyperparameter optimization—adjusting variables like learning rates, layer widths, and dropout thresholds to ensure the best possible accuracy. Regularization techniques such as **BatchNormalization** and **Dropout** were used to prevent overfitting and improve generalization across unseen races.

One unique component of the pipeline is a **recency bias decay function**, which weights recent race results more heavily than older ones. This ensures the model stays aligned with the dynamic nature of the sport, where driver momentum and mid-season upgrades can significantly alter performance.

To improve training efficiency and model convergence, I used callbacks like **EarlyStopping** and **ReduceLROnPlateau**, as well as a **custom loss threshold** callback that halts training when a specific validation target is met.

The goal of this project is not just prediction, but interpretability. I want the model to provide **insight** into what’s driving the outcome—whether it’s a driver's form, track specialization, or team consistency. It continues to challenge me with tasks like data normalization, feature engineering, and real-world evaluation, while keeping me deeply connected to a sport I love.



### **Blue Sky Solar Racing (BSSR)**

As the **Chief Structural and Manufacturing Engineer** for the University of Toronto’s Blue Sky Solar Racing Team, I led the design and fabrication of the solar car body for the World Solar Challenge—a 3,000+ km race across the Australian Outback that pushes the limits of energy efficiency and engineering endurance.

My role centered on optimizing the structural integrity and manufacturability of the car’s composite shell. I spearheaded the plug and mold fabrication process, focusing on reducing seamlines and streamlining layup workflows. Through iterative improvements in resin flow and mold construction, I helped **cut material waste by 17%**, accelerating production and improving surface quality.

I oversaw the infusion of **carbon fiber and kevlar layups**, running tests to refine flow rates and curing conditions. I also managed our **3D printing operations**, optimizing print profiles to **reduce build times by 20%** while maintaining strength and precision. These components were used for aerodynamic add-ons and interior fitments.

Throughout the project, I collaborated closely with the **electrical**, **solar array**, and **strategy** sub-teams to ensure mechanical decisions aligned with performance targets. From shell stiffness to mounting interfaces, every choice was made to balance durability, weight, and aerodynamic performance.

This role tested my ability to deliver under tight deadlines and taught me how to lead in high-stakes environments. Beyond the technical achievements, it showed me the power of hands-on collaboration, iterative design, and mission-driven teamwork. Our car completed the race, representing not just our university, but our collective belief in sustainable engineering.



### **Robotics for Space Exploration (RSX) Rover Team**

I was a core member of the **Robotics for Space Exploration (RSX)** Design Team, contributing to the development of planetary rovers for competitions such as the **Canadian International Rover Challenge (CIRC)** and the **University Rover Challenge (URC)**. My role focused on autonomy and software, where I gained hands-on experience with sensor integration, real-time perception, and field-ready system deployment.

For **CIRC 2022**, I led the implementation of the rover’s **LiDAR** and **IMU** systems. This involved configuring hardware settings, writing ROS nodes, and synchronizing data streams to create a robust perception pipeline. The LiDAR proved instrumental during the science task, generating detailed topographical maps that improved our terrain classification and **boosted our science task performance by 20%**.

I also contributed to the development of the rover’s **robotic arm**, writing and testing **Arduino code** for joint control and feedback loops. This required careful calibration and collaboration with the mechanical team to ensure safe actuation and precision handling during object manipulation challenges.

In addition, I set up a **dedicated Ubuntu server** on one of our base laptops to run ROS and manage multi-process communication, giving the software team a stable environment for testing autonomy algorithms. I also explored forward-looking improvements for future competitions, researching SLAM-based navigation and object detection modules.

This project taught me how to develop real-time robotic systems under field conditions—balancing performance with reliability while debugging on the fly. It deepened my understanding of ROS, sensor fusion, and autonomous system design, and gave me a strong appreciation for what it takes to make robots operate in the wild.


### **EWB Curriculum Reform**

During my time with **Engineers Without Borders (EWB)**, I led the **Curriculum Change Project**—an initiative aimed at integrating sustainability more deeply into the **Engineering Strategies & Practice (ESP)** course at the University of Toronto. What began as a broad ambition became a focused, high-impact reform effort under my leadership.

After taking over the project, I **rescaled the scope** to concentrate specifically on the ESP course, which touches nearly every first-year engineering student. I **recruited and mentored new team members**, conducted a comparative analysis of sustainability curricula at other institutions, and collaborated with the **Sustainability Engineering Association (SEA)** to launch a campus-wide survey. This survey captured valuable student perspectives on how sustainability should be represented in foundational engineering education.

We synthesized our findings into a **comprehensive policy report** that outlined actionable strategies for integrating sustainability into course content, learning objectives, and project frameworks. I also engaged with course instructors and program leadership to review feasibility and implementation pathways, advocating for reforms that were both ambitious and pragmatic.

Even after stepping down as team lead, I remained involved as a **consultant**, offering continuity and strategic guidance as the reforms moved toward adoption. The project significantly enhanced my abilities in policy writing, stakeholder negotiation, and collaborative problem-solving across institutional structures.

Ultimately, this experience taught me how to **translate vision into institutional change**—and reinforced my belief that engineers should be equipped not just with technical skills, but also with the tools to address social and environmental challenges head-on.



### **Thesis: Optimizing 3D Metal Printing Using Machine Learning**

My undergraduate thesis presents a comprehensive framework to improve the reliability and quality of **Laser Directed Energy Deposition (LDED)**—an advanced metal additive manufacturing process—by integrating high-speed thermal imaging with predictive machine learning models.

LDED enables the fabrication and repair of complex, high-value metal components, but its precision is often limited by the dynamic and unstable behavior of the melt pool. To address this, I designed a **high-throughput experimental setup** that systematically varied over **360 combinations** of laser power, scan speed, and powder feed rate. Each print trial was monitored in real time using a **2000 fps infrared camera**, enabling the capture of thermal dynamics throughout the deposition process.

From this data, I extracted both **dynamic features** (e.g., melt pool stability, morphology, sputter density) and **static features** (e.g., track height, volume, surface roughness). Melt pool stability—quantified via steady-state duration and the coefficient of variation—emerged as the most consistent real-time predictor of print quality. Conversely, morphology and sputter density, though visually distinctive, showed poor correlation with geometric output and were excluded from further modeling.

Using these features, I trained and evaluated four regression models to predict key quality indicators:

* **Linear Regression**
* **Decision Trees**
* **Extra Trees Ensemble**
* **Neural Networks**

These models were tested against target outcomes such as melt track height, melt pool area, and a hybrid quality score combining stability and surface roughness. Neural networks and ensemble tree models achieved the highest performance, with **R² scores exceeding 0.84**, demonstrating that hybrid feature sets offer significant predictive advantage over using static or dynamic features alone.

**Key findings include**:

* **Scan speed and feed rate** were more influential than laser power in determining print quality.
* **Stable prints** were most common at high-power, low-scan-speed settings.
* **Melt pool stability** consistently outperformed traditional post-process geometry as an early indicator of defects.

This thesis not only deepens our understanding of LDED physics but also lays the groundwork for **real-time quality monitoring and closed-loop control**. The integration of high-speed IR sensing and interpretable ML models moves us closer to adaptive, intelligent metal printing systems—offering significant impact in aerospace, biomedical, and industrial manufacturing.



### **Mold Masters – Automation Intern**

During my Professional Experience Year (PEY), I worked as an **Automation Intern** at **Mold Masters**, where I designed and deployed custom automation tools to streamline CAD model creation and optimize data workflows for the manufacturing team. My primary focus was building solutions that reduced engineering time, improved tool usability, and supported data-driven decision-making across departments.

I rapidly upskilled in **VB.NET** to develop internal tools that automated repetitive CAD processes—cutting down the time engineers spent generating models and freeing them to focus on higher-value design tasks. These automations became core to the team’s workflow, improving consistency and reducing manual error.

I emphasized **user-centric design**, regularly collecting feedback from engineers and designers to iterate on features and ensure the tools fit their real-world needs. This not only boosted adoption but also improved team productivity and satisfaction.

In parallel, I built **SQL-based automation scripts** that extracted key metrics from internal databases and generated **automated reports** for project managers. These real-time dashboards improved visibility into part performance, order progress, and production timelines—helping stakeholders make faster, more informed decisions.

My work spanned multiple departments, requiring cross-functional communication to align tool features with operational goals. I collaborated with manufacturing, design, and management teams to ensure that automation wasn’t just technically sound but strategically impactful.

Due to my contributions, I was invited to continue part-time after my PEY ended—maintaining, refining, and expanding the automation ecosystem I helped build. This role sharpened my skills in **programming**, **process optimization**, and **interdepartmental collaboration**, while making a tangible impact on engineering efficiency at scale.

